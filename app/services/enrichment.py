import logging
import asyncio
import httpx
import json
from typing import List, Optional, Dict, Any, Tuple
from app.config import settings
from app.models import Keyword, SerpResult
from app.database import db_manager

logger = logging.getLogger(__name__)

class EnrichmentService:
    def __init__(self):
        self.datafor_seo_credentials = self._get_datafor_seo_credentials()
    
    def _get_datafor_seo_credentials(self) -> Optional[Dict[str, str]]:
        """R√©cup√®re les credentials DataForSEO depuis .env"""
        login = settings.datafor_seo_login
        password = settings.datafor_seo_password
        
        if login and password:
            return {"login": login, "password": password}
        return None
    
    async def enrich_keywords_metrics(self, keywords: List[Keyword]) -> List[Keyword]:
        """Enrichit les mots-cl√©s avec les m√©triques SEO"""
        logger.info(f"üí∞ D√©but de l'enrichissement de {len(keywords)} mots-cl√©s")
        
        # Les m√©triques sont maintenant directement lues depuis le fichier CSV
        # On ne fait plus d'enrichissement automatique
        enriched_keywords = []
        
        logger.debug(f"üìä √âchantillon des mots-cl√©s √† traiter:")
        for i, kw in enumerate(keywords[:3]):
            logger.debug(f"  {i+1}. {kw.keyword} (vol: {kw.search_volume}, cpc: {kw.cpc})")
        
        for i, kw in enumerate(keywords):
            # Garde les donn√©es existantes du fichier
            enriched_kw = kw.model_copy()
            enriched_keywords.append(enriched_kw)
            
            if i % 100 == 0:  # Log tous les 100 mots-cl√©s
                logger.debug(f"‚öôÔ∏è Trait√© {i+1}/{len(keywords)} mots-cl√©s")
        
        logger.info(f"‚úÖ Enrichissement termin√© pour {len(enriched_keywords)} mots-cl√©s (donn√©es du fichier utilis√©es)")
        return enriched_keywords
    
    async def scrape_serp_results(self, keyword: str, country_code: str = "FR") -> List[SerpResult]:
        """Scrape les r√©sultats SERP pour un mot-cl√© avec DataForSEO"""
        # V√©rifier le cache d'abord
        cached_results = db_manager.get_cached_serp_results(keyword, country_code)
        if cached_results:
            logger.debug(f"R√©sultats SERP trouv√©s en cache pour '{keyword}'")
            return cached_results
        
        logger.info(f"Scraping SERP DataForSEO pour '{keyword}' ({country_code})")
        
        try:
            if self.datafor_seo_credentials:
                results = await self._scrape_with_datafor_seo(keyword, country_code)
            else:
                logger.warning("Aucun credential DataForSEO configur√©")
                return []
            
            # Mettre en cache les r√©sultats
            if results:
                db_manager.cache_serp_results(
                    keyword, 
                    country_code, 
                    results, 
                    settings.default_serp_cache_ttl_days
                )
            
            return results
            
        except Exception as e:
            logger.error(f"Erreur lors du scraping SERP pour '{keyword}': {e}")
            return []
    
    async def _scrape_with_datafor_seo(self, keyword: str, country_code: str) -> List[SerpResult]:
        """Scrape avec DataForSEO API"""
        try:
            # Configuration DataForSEO
            base_url = "https://api.dataforseo.com/v3/serp/google/organic/live/advanced"
            
            # Payload pour l'API DataForSEO
            payload = [{
                "keyword": keyword,
                "location_code": self._get_location_code(country_code),
                "language_code": "fr",
                "device": "desktop",
                "os": "windows"
            }]
            
            # Authentification Basic Auth
            auth = (
                self.datafor_seo_credentials["login"],
                self.datafor_seo_credentials["password"]
            )
            
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    base_url,
                    json=payload,
                    auth=auth,
                    timeout=30.0
                )
                
                if response.status_code != 200:
                    logger.error(f"Erreur DataForSEO HTTP {response.status_code}: {response.text}")
                    return []
                
                try:
                    data = response.json()
                except Exception as e:
                    logger.error(f"Erreur lors du parsing JSON DataForSEO: {e}")
                    return []
                
                # Parse les r√©sultats
                serp_results = []
                if data.get("status_code") == 20000 and data.get("tasks"):
                    task_result = data["tasks"][0]
                    if task_result.get("status_code") == 20000 and task_result.get("result"):
                        organic_results = task_result["result"][0].get("items", [])
                        
                        for i, result in enumerate(organic_results, 1):
                            if result.get("type") == "organic":
                                # Protection ultra-robuste contre les valeurs None et autres types
                                description = result.get("description")
                                if description is None:
                                    description = ""
                                elif not isinstance(description, str):
                                    description = str(description) if description else ""
                                
                                # Protection pour les autres champs √©galement
                                url = result.get("url")
                                if url is None:
                                    url = ""
                                elif not isinstance(url, str):
                                    url = str(url) if url else ""
                                
                                title = result.get("title")
                                if title is None:
                                    title = ""
                                elif not isinstance(title, str):
                                    title = str(title) if title else ""
                                
                                # Log de debug pour comprendre ce qui se passe
                                logger.debug(f"üîç Cr√©ation SerpResult pour '{keyword}' - description: {repr(description)}, type: {type(description)}")
                                
                                # Cr√©ation du SerpResult avec donn√©es nettoy√©es
                                try:
                                    # Double v√©rification avant cr√©ation
                                    clean_data = {
                                        "keyword": str(keyword),
                                        "url": str(url),
                                        "title": str(title),
                                        "description": str(description),
                                        "position": int(i),
                                        "domain": self._extract_domain(str(url))
                                    }
                                    
                                    serp_result = SerpResult(**clean_data)
                                    serp_results.append(serp_result)
                                    logger.debug(f"‚úÖ SerpResult cr√©√© avec succ√®s pour '{keyword}' - position {i}")
                                    
                                except Exception as e:
                                    logger.error(f"‚ùå Erreur lors de la cr√©ation de SerpResult pour '{keyword}': {e}")
                                    logger.error(f"   Donn√©es: {clean_data}")
                                    continue
                
                logger.info(f"‚úÖ DataForSEO: {len(serp_results)} r√©sultats pour '{keyword}'")
                return serp_results
            
        except Exception as e:
            logger.error(f"Erreur DataForSEO pour '{keyword}': {e}")
            return []
    
    def _get_location_code(self, country_code: str) -> int:
        """Convertit un code pays en location_code DataForSEO"""
        # Mapping des codes pays vers les location codes DataForSEO
        location_codes = {
            "FR": 2250,  # France
            "US": 2840,  # United States
            "UK": 2826,  # United Kingdom
            "DE": 2276,  # Germany
            "ES": 2724,  # Spain
            "IT": 2380,  # Italy
            "BE": 2056,  # Belgium
            "CH": 2756,  # Switzerland
            "CA": 2124,  # Canada
        }
        return location_codes.get(country_code.upper(), 2250)  # Default: France
    
    def _extract_domain(self, url: str) -> str:
        """Extrait le domaine d'une URL"""
        try:
            from urllib.parse import urlparse
            return urlparse(url).netloc
        except:
            return ""
    
    async def calculate_serp_similarity(
        self, 
        keywords: List[str], 
        country_code: str = "FR",
        max_concurrent: int = 10,
        enable_sampling: bool = False,
        sampling_ratio: float = 0.3,
        enable_smart_clustering: bool = True
    ) -> Dict[str, Dict[str, float]]:
        """Calcule la similarit√© entre les SERPs de diff√©rents mots-cl√©s avec optimisations"""
        logger.info(f"üöÄ Calcul de la similarit√© SERP pour {len(keywords)} mots-cl√©s avec DataForSEO")
        logger.info(f"‚öôÔ∏è Optimisations: concurrent={max_concurrent}, sampling={enable_sampling}, smart_clustering={enable_smart_clustering}")
        
        # üéØ OPTIMISATION 1: √âchantillonnage intelligent si activ√©
        keywords_to_process = keywords.copy()
        if enable_sampling and len(keywords) > 100:
            import random
            sample_size = max(50, int(len(keywords) * sampling_ratio))
            
            # √âchantillonnage strat√©gique: garde les mots-cl√©s importants
            # Tri par longueur pour garder des mots-cl√©s repr√©sentatifs
            sorted_keywords = sorted(keywords, key=len)
            step = len(sorted_keywords) // sample_size
            keywords_to_process = [sorted_keywords[i] for i in range(0, len(sorted_keywords), max(1, step))][:sample_size]
            
            logger.info(f"üìä √âchantillonnage: {len(keywords_to_process)}/{len(keywords)} mots-cl√©s s√©lectionn√©s")
        
        # üéØ OPTIMISATION 2: Clustering pr√©liminaire pour √©viter les doublons proches
        if enable_smart_clustering and len(keywords_to_process) > 50:
            keywords_to_process = await self._smart_keyword_selection(keywords_to_process)
            logger.info(f"üß† S√©lection intelligente: {len(keywords_to_process)} mots-cl√©s repr√©sentatifs")
        
        # üöÄ OPTIMISATION 3: Appels parall√®les avec semaphore pour limiter la concurrence
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def scrape_with_limit(keyword: str):
            async with semaphore:
                try:
                    results = await self.scrape_serp_results(keyword, country_code)
                    domains = [r.domain for r in results[:10]]  # Top 10 results
                    return keyword, set(domains)
                except Exception as e:
                    logger.error(f"‚ùå Erreur SERP pour '{keyword}': {e}")
                    return keyword, set()
        
        # Lancement de tous les appels en parall√®le
        logger.info(f"üîÑ Lancement de {len(keywords_to_process)} appels SERP parall√®les...")
        tasks = [scrape_with_limit(keyword) for keyword in keywords_to_process]
        
        # Traitement par batches pour √©viter l'overload
        batch_size = 50
        serp_data = {}
        
        for i in range(0, len(tasks), batch_size):
            batch_tasks = tasks[i:i+batch_size]
            logger.info(f"üì¶ Traitement batch {i//batch_size + 1}/{(len(tasks)-1)//batch_size + 1}")
            
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            for result in batch_results:
                if isinstance(result, Exception):
                    logger.error(f"‚ùå Erreur dans le batch: {result}")
                    continue
                keyword, domains = result
                serp_data[keyword] = domains
        
        logger.info(f"‚úÖ R√©cup√©ration SERP termin√©e: {len(serp_data)} mots-cl√©s trait√©s")
        
        # üéØ OPTIMISATION 4: Extension de la matrice pour les mots-cl√©s non trait√©s
        if enable_sampling or enable_smart_clustering:
            serp_data = await self._extend_similarity_matrix(keywords, serp_data, keywords_to_process)
        
        # Calcule la similarit√© Jaccard entre chaque paire (optimis√©)
        similarity_matrix = self._calculate_jaccard_matrix_optimized(keywords, serp_data)
        
        logger.info(f"‚úÖ Matrice de similarit√© SERP calcul√©e pour {len(keywords)} mots-cl√©s")
        return similarity_matrix
    
    async def _smart_keyword_selection(self, keywords: List[str]) -> List[str]:
        """S√©lection intelligente de mots-cl√©s repr√©sentatifs"""
        try:
            # Clustering rapide bas√© sur la similarit√© textuelle
            from sentence_transformers import SentenceTransformer
            import numpy as np
            from sklearn.cluster import KMeans
            from sklearn.metrics.pairwise import cosine_similarity
            
            # Utilise un mod√®le plus petit pour la s√©lection rapide
            model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')
            embeddings = model.encode(keywords, show_progress_bar=False)
            
            # Clustering en groupes repr√©sentatifs
            # Objectif : r√©duire √† environ 30-50% des mots-cl√©s originaux
            target_reduction = 0.4  # Garde 40% des mots-cl√©s
            n_clusters = max(5, int(len(keywords) * target_reduction))
            n_clusters = min(n_clusters, len(keywords) - 1)  # Au maximum len(keywords)-1 clusters
            
            logger.debug(f"üß† Clustering {len(keywords)} mots-cl√©s en {n_clusters} groupes repr√©sentatifs")
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(embeddings)
            
            # S√©lectionne le repr√©sentant de chaque cluster (plus proche du centro√Øde)
            selected_keywords = []
            for cluster_id in range(n_clusters):
                cluster_mask = cluster_labels == cluster_id
                cluster_embeddings = embeddings[cluster_mask]
                cluster_keywords = [kw for i, kw in enumerate(keywords) if cluster_mask[i]]
                
                if len(cluster_keywords) > 0:
                    # Trouve le mot-cl√© le plus proche du centro√Øde
                    centroid = cluster_embeddings.mean(axis=0).reshape(1, -1)
                    similarities = cosine_similarity(cluster_embeddings, centroid).flatten()
                    best_idx = similarities.argmax()
                    selected_keywords.append(cluster_keywords[best_idx])
            
            return selected_keywords
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur s√©lection intelligente: {e}, utilisation de l'√©chantillonnage simple")
            # Fallback: √©chantillonnage simple
            import random
            sample_size = min(50, len(keywords))
            return random.sample(keywords, sample_size)
    
    async def _extend_similarity_matrix(
        self, 
        all_keywords: List[str], 
        serp_data: Dict[str, set], 
        processed_keywords: List[str]
    ) -> Dict[str, set]:
        """√âtend la matrice de similarit√© par approximation pour les mots-cl√©s non trait√©s"""
        try:
            from sentence_transformers import SentenceTransformer
            from sklearn.metrics.pairwise import cosine_similarity
            import numpy as np
            
            # Mots-cl√©s non trait√©s
            missing_keywords = [kw for kw in all_keywords if kw not in serp_data]
            
            if not missing_keywords:
                return serp_data
            
            logger.info(f"üîÆ Extension par approximation pour {len(missing_keywords)} mots-cl√©s")
            
            # Calcule les embeddings pour l'approximation
            model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')
            processed_embeddings = model.encode(processed_keywords, show_progress_bar=False)
            missing_embeddings = model.encode(missing_keywords, show_progress_bar=False)
            
            # Pour chaque mot-cl√© manquant, trouve le plus similaire dans les trait√©s
            similarities = cosine_similarity(missing_embeddings, processed_embeddings)
            
            extended_serp_data = serp_data.copy()
            for i, missing_kw in enumerate(missing_keywords):
                # Trouve le mot-cl√© trait√© le plus similaire
                best_match_idx = similarities[i].argmax()
                best_match_keyword = processed_keywords[best_match_idx]
                
                # Copie les domaines du mot-cl√© le plus similaire avec un facteur de bruit
                original_domains = serp_data[best_match_keyword]
                
                # Ajoute un peu de variation pour simuler la diff√©rence
                similarity_score = similarities[i][best_match_idx]
                keep_ratio = 0.6 + (similarity_score * 0.3)  # Entre 60% et 90% des domaines
                
                import random
                domains_to_keep = random.sample(
                    list(original_domains), 
                    max(1, int(len(original_domains) * keep_ratio))
                )
                
                extended_serp_data[missing_kw] = set(domains_to_keep)
            
            logger.info(f"‚úÖ Extension termin√©e: {len(extended_serp_data)} mots-cl√©s au total")
            return extended_serp_data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur extension matrice: {e}")
            # Fallback: domaines vides pour les mots-cl√©s manquants
            extended_serp_data = serp_data.copy()
            missing_keywords = [kw for kw in all_keywords if kw not in serp_data]
            for kw in missing_keywords:
                extended_serp_data[kw] = set()
            return extended_serp_data
    
    def _calculate_jaccard_matrix_optimized(
        self, 
        keywords: List[str], 
        serp_data: Dict[str, set]
    ) -> Dict[str, Dict[str, float]]:
        """Calcul optimis√© de la matrice de similarit√© Jaccard"""
        similarity_matrix = {}
        
        # Pr√©-calcule les donn√©es pour √©viter les recalculs
        keyword_domains = {kw: serp_data.get(kw, set()) for kw in keywords}
        
        for i, kw1 in enumerate(keywords):
            similarity_matrix[kw1] = {}
            domains1 = keyword_domains[kw1]
            
            for j, kw2 in enumerate(keywords):
                if i == j:
                    similarity_matrix[kw1][kw2] = 1.0
                elif kw2 in similarity_matrix and kw1 in similarity_matrix[kw2]:
                    # Utilise la sym√©trie pour √©viter le recalcul
                    similarity_matrix[kw1][kw2] = similarity_matrix[kw2][kw1]
                else:
                    # Calcule la similarit√© Jaccard
                    domains2 = keyword_domains[kw2]
                    intersection = len(domains1.intersection(domains2))
                    union = len(domains1.union(domains2))
                    similarity = intersection / union if union > 0 else 0.0
                    similarity_matrix[kw1][kw2] = similarity
        
        return similarity_matrix

    async def batch_scrape_serp_results(
        self, 
        keywords: List[str], 
        country_code: str = "FR",
        max_concurrent: int = 10
    ) -> Dict[str, List[SerpResult]]:
        """R√©cup√®re les r√©sultats SERP pour plusieurs mots-cl√©s en parall√®le avec gestion du cache"""
        logger.info(f"üîç Batch SERP pour {len(keywords)} mots-cl√©s")
        
        # S√©pare les mots-cl√©s cach√©s et non-cach√©s
        cached_results = {}
        keywords_to_scrape = []
        
        for keyword in keywords:
            cached = db_manager.get_cached_serp_results(keyword, country_code)
            if cached:
                cached_results[keyword] = cached
                logger.debug(f"‚úÖ Cache hit pour '{keyword}'")
            else:
                keywords_to_scrape.append(keyword)
        
        logger.info(f"üìä Cache: {len(cached_results)} hits, {len(keywords_to_scrape)} mots-cl√©s √† scraper")
        
        if not keywords_to_scrape:
            return cached_results
        
        # Scraping parall√®le des mots-cl√©s manquants
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def scrape_single(keyword: str) -> Tuple[str, List[SerpResult]]:
            async with semaphore:
                results = await self.scrape_serp_results(keyword, country_code)
                return keyword, results
        
        # Lance tous les appels en parall√®le
        tasks = [scrape_single(keyword) for keyword in keywords_to_scrape]
        scraped_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Combine les r√©sultats
        all_results = cached_results.copy()
        
        for result in scraped_results:
            if isinstance(result, Exception):
                logger.error(f"‚ùå Erreur batch scraping: {result}")
                continue
            keyword, serp_results = result
            all_results[keyword] = serp_results
        
        logger.info(f"‚úÖ Batch termin√©: {len(all_results)} mots-cl√©s trait√©s")
        return all_results

# Instance globale du service d'enrichissement
enrichment_service = EnrichmentService() 